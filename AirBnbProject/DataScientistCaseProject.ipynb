# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.simplefilter(action='ignore')

# %%
df = pd.read_csv("listings.csv")
df

# %%
#1.What is the correlation among the columns 'price,' 'minimum_nights,' 'number_of_reviews,' 'availability_365,' and 'number_of_reviews_ltm'?
correlation_set = df[["price", "minimum_nights", "number_of_reviews", "availability_365", "number_of_reviews_ltm"]]
correlation_matrix = correlation_set.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix of Listing Attributes')
plt.show()

# %%
#2. What are min/max and average rental price ?
df = df[df['price'] != 0]
#Price column check
price = df["price"]
price.isna().sum()
price.isnull().sum()
price.info()



# %%
min_price = df['price'].min()
max_price = df['price'].max()
average_price = df['price'].mean()

print('Minimum price %d$.' % (min_price))
print('Maximum price %d$' % (max_price))
print('Average price %d$.' % (mean_price))

# %%
#3.What is average price by room types )
#Check columns
room_type=df["room_type"]
(room_type.isna().sum()) + (room_type.isnull().sum())

# %%
# Calculate the average prices by grouping the data according to the 'room_type' column
average_prices_by_room_type = df.groupby('room_type')['price'].mean()

print(average_prices_by_room_type)

average_prices_by_room_type.plot(kind='bar', color='gray')
plt.title('Average Prices By Room Type')
plt.xlabel('Room Type')
plt.ylabel('Average Rent Price')
plt.show()

# %%
#4.What are the most expensive and cheapest regions geographically?
import folium
from folium.plugins import HeatMap


# Calculate the difference from the average price
df['price_difference'] = df['price'] - average_price

# Create a base map centered around London
map_center = [51.509865, -0.118092]  # Coordinates for London
london_map = folium.Map(location=map_center, zoom_start=11)

# Create a colormap function
colormap = folium.LinearColormap(colors=['green', 'blue', 'red'], vmin=-100, vmax=100)

# Create HeatMap layer for regional price distribution
heat_data = df[['latitude', 'longitude', 'price_difference']]
heat_data = heat_data.dropna(subset=['latitude', 'longitude', 'price_difference'])
heat_array = heat_data.values

# Apply colormap to price differences
colors = [colormap(x) for x in heat_data['price_difference']]

# Create a feature group for the HeatMap layer
heat_group = folium.FeatureGroup(name='HeatMap')

# Add each point to the HeatMap layer with its corresponding color
for lat, lon, color in zip(heat_data['latitude'], heat_data['longitude'], colors):
    folium.CircleMarker([lat, lon], radius=5, color=None, fill_color=color, fill_opacity=0.7).add_to(heat_group)

# Add HeatMap layer to the map
london_map.add_child(heat_group)
london_map.add_child(colormap)  # Add colormap to the map

# Display the map in Jupyter Notebook
london_map

# %%
# 5. What is the most expensive and cheapest neighbourhood?

# Extracting the 'neighbourhood' column from the DataFrame
neighbourhood = df["neighbourhood"]

# Checking for missing values in the 'neighbourhood' column
(neighbourhood.isnull()).sum() + (neighbourhood.isna().sum())

# Creating a subset with 'neighbourhood' and 'price' columns
neighbourhood_price_subset = df[['neighbourhood', 'price']]

# Calculating the average prices for each neighbourhood
average_prices_by_neighbourhood = neighbourhood_price_subset.groupby('neighbourhood')['price'].mean().sort_values(ascending=False)

# Selecting the most expensive and cheapest neighbourhoods
expensive_cheapest_neighbourhoods = average_prices_by_neighbourhood.head(35)

# Visualization of the top 5 most expensive neighbourhoods
plt.figure(figsize=(14, 8))
sns.barplot(x=expensive_cheapest_neighbourhoods.values, y=expensive_cheapest_neighbourhoods.index, hue=expensive_cheapest_neighbourhoods.index, palette='Reds_r', legend=False)
plt.title('Top 5 Most Expensive Neighbourhoods')
plt.xlabel('Average Price')
plt.ylabel('Neighbourhood')
plt.show()

# Printing the most expensive and cheapest neighbourhoods
print("Most Expensive Neighbourhood is Westminster")
print("Cheapest Neighbourhood is Sutton")

# %%
# 6. Impact of minimum nights on reviews
# Exploring the data
subset = df[['minimum_nights', 'number_of_reviews']].copy()

# Visualizing the distribution
plt.figure(figsize=(12, 8))
sns.scatterplot(data=subset, x='minimum_nights', y='number_of_reviews')
plt.title('Scatter Plot of Minimum Nights vs. Reviews')
plt.xlabel('Minimum Nights')
plt.ylabel('Reviews')
plt.show()

# Grouping and statistical analysis
bins = [0, 7, 14, 30, 365]
labels = ['1-7 nights', '8-14 nights', '15-30 nights', '31+ nights']
subset['minimum_nights_group'] = pd.cut(subset['minimum_nights'], bins=bins, labels=labels, right=False)
grouped_means = subset.groupby('minimum_nights_group')['number_of_reviews'].mean().sort_values()

# Visualization
plt.figure(figsize=(12, 8))
sns.barplot(x=grouped_means.values, y=grouped_means.index, palette='viridis')
plt.title('Average Reviews by Minimum Nights Group')
plt.xlabel('Average Reviews')
plt.ylabel('Minimum Nights Group')
plt.show()

# %%
# 7.What are the most common used words in the listing ?
from collections import Counter

names = df["name"]
(names.isna().sum()) + (names.isnull().sum())

# %%
import re
from nltk.tokenize import word_tokenize
# Concatenate all words in the 'name' column
all_names = ' '.join(df['name'].astype(str))

# Tokenize the words
tokenized_names = word_tokenize(all_names)

# Find words that do not contain numbers or punctuation marks
filtered_words = [word for word in tokenized_names if word.isalpha() and not bool(re.search(r'\d', word))]

# Count the occurrences of each word
word_counts = Counter(filtered_words)

# Find the most common words
top_words = word_counts.most_common(20)

# Convert the results to a DataFrame for better plotting
top_words_df = pd.DataFrame(top_words, columns=['Word', 'Count'])

# Plot the bar chart
plt.figure(figsize=(22, 8))
sns.barplot(x='Word', y='Count', data=top_words_df, palette='viridis')
plt.title('Count of Words')
plt.xlabel('Most Common 20 Words in Listing Names')
plt.show()

# %%
from sklearn.linear_model import LinearRegression
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# %%
#Lineer Regression Model for Prediction

# Drop unnecessary columns for model
columns_to_drop = ['host_id', 'latitude', 'longitude', 'neighbourhood_group', 'reviews_per_month']
df.drop(columns=columns_to_drop, inplace=True)

# One Hot Encoding
df_encoded = pd.get_dummies(df, columns=['neighbourhood', 'room_type'])

# Defining the independent variables and dependent variable
x = df_encoded.drop(columns=['price'])  
y = df_encoded['price'] 

# Split the dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Separate numeric and categorical columns
numeric_cols = x_train.select_dtypes(include=['float64', 'int64']).columns
categorical_cols = x_train.select_dtypes(include=['object']).columns

# Create a preprocessor to handle numeric and categorical columns separately
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_cols),  # Using StandardScaler for numeric columns
        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)
    ])

# Fit and transform the preprocessor on X_train
x_train_preprocessed = preprocessor.fit_transform(x_train)

# Initialize and fit the linear regression model
reg = LinearRegression()
reg.fit(x_train_preprocessed, y_train)

# Transform X_test using the same preprocessor
x_test_preprocessed = preprocessor.transform(x_test)

# Make predictions on the test set
y_pred = reg.predict(x_test_preprocessed)

# Evaluate the model
r2_score_value = r2_score(y_test, y_pred)
print(f'R-squared score: {r2_score_value}')

# %%
#8. What are the real prices and predicted prices ?
y_pred = reg.predict(x_test_preprocessed)
results = pd.DataFrame({'Real Prices': y_test, 'Predicted Prices': y_pred})

plt.figure(figsize=(12, 6))
plt.plot(y_test, y_pred)
plt.xlabel('Real Prices',fontsize=16)
plt.ylabel('Predicted Prices', fontsize=16)
plt.title('Real Prices vs. Predicted Prices',fontsize=20)
plt.show()

results = pd.DataFrame({'Real Prices': y_test, 'Predicted Prices': y_pred})
print("Real Prices and Predicted Prices:")
print(results)

# %%
#9. What are types of homes and their prevalence in neighborhoods?
plt.figure(figsize=(14, 8))

# Create a count plot to visualize the prevalence of different room types in each neighborhood
sns.countplot(x='neighbourhood', hue='room_type', data=df)

# Set the title and labels for the plot
plt.title('Prevalence of Home Types in Neighborhoods')
plt.xlabel('Neighborhood')
plt.ylabel('Number of Homes')

# Rotate the x-axis labels for better readability
plt.xticks(rotation=45, ha='right')

# Add a legend to represent room types and adjust its position
plt.legend(title='Room Type', bbox_to_anchor=(1.05, 1), loc='upper left')

# Adjust layout for better presentation
plt.tight_layout()

# Show the plot
plt.show()

room_type_prevalence = df.groupby(['neighbourhood', 'room_type']).size().unstack()
room_type_prevalence.fillna(0, inplace=True)

# Print the result
print("Prevalence of Room Types in Different Neighborhoods:")
print(room_type_prevalence)

# %%
#10. What is the overall availability trend throughout the year and how does it quantitatively relate to the average price?
#Visualization
plt.figure(figsize=(12, 6))

# Create a line plot to visualize the relationship between availability throughout the year and average price
sns.lineplot(x='availability_365', y='price', data=df)

# Set the title and labels for the plot
plt.title('Relationship between Availability Throughout the Year and Price')
plt.xlabel('Availability Days Throughout the Year')
plt.ylabel('Average Price')

# Show the plot
plt.show()

availability_price_correlation = round(df['availability_365'].corr(df['price']), 2)
print(f"The correlation between availability throughout the year and average price is: {availability_price_correlation}")


